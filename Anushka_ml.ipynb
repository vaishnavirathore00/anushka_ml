{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b741758d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa91dafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "57020058",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The predicted value using Decision tree{} [1 0 1 1 1 0 1 1 1 1 1 0 0 0 0 1 1 1 1 1 0 1 0 1 1 1 1 1 0 0]\n",
      "Accuracyof the model: 0.6333333333333333\n"
     ]
    }
   ],
   "source": [
    "class DecisionTree:\n",
    "    def __init__(self, max_depth=None):\n",
    "        self.max_depth = max_depth\n",
    "        self.tree = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.tree = self._fit(X, y, depth=0)\n",
    "\n",
    "    def _fit(self, X, y, depth):\n",
    "        unique_classes, counts = np.unique(y, return_counts=True)\n",
    "\n",
    "        # If only one class or max depth reached, create a leaf node\n",
    "        if len(unique_classes) == 1 or (self.max_depth is not None and depth == self.max_depth):\n",
    "            return {'class': unique_classes[0], 'count': counts[0]}\n",
    "\n",
    "        # Find the best split\n",
    "        best_split = self._find_best_split(X, y)\n",
    "\n",
    "        if best_split is None:\n",
    "            return {'class': unique_classes[np.argmax(counts)], 'count': counts[0]}\n",
    "\n",
    "        feature_index, threshold = best_split\n",
    "        left_mask = X[:, feature_index] <= threshold\n",
    "        right_mask = ~left_mask\n",
    "\n",
    "        # Recursively build the left and right subtrees\n",
    "        left_subtree = self._fit(X[left_mask], y[left_mask], depth + 1)\n",
    "        right_subtree = self._fit(X[right_mask], y[right_mask], depth + 1)\n",
    "\n",
    "        return {'feature_index': feature_index, 'threshold': threshold,\n",
    "                'left': left_subtree, 'right': right_subtree}\n",
    "\n",
    "    def _find_best_split(self, X, y):\n",
    "        m, n = X.shape\n",
    "        if m <= 1:\n",
    "            return None\n",
    "\n",
    "        num_classes = len(np.unique(y))\n",
    "        if num_classes == 1:\n",
    "            return None\n",
    "\n",
    "        # Calculate the impurity before the split\n",
    "        base_impurity = self._calculate_impurity(y)\n",
    "\n",
    "        best_impurity_reduction = 0\n",
    "        best_split = None\n",
    "\n",
    "        for feature_index in range(n):\n",
    "            thresholds = np.unique(X[:, feature_index])\n",
    "            for threshold in thresholds:\n",
    "                left_mask = X[:, feature_index] <= threshold\n",
    "                right_mask = ~left_mask\n",
    "\n",
    "                if np.sum(left_mask) > 0 and np.sum(right_mask) > 0:\n",
    "                    left_impurity = self._calculate_impurity(y[left_mask])\n",
    "                    right_impurity = self._calculate_impurity(y[right_mask])\n",
    "\n",
    "                    # Weighted impurity reduction\n",
    "                    impurity_reduction = base_impurity - (np.sum(left_mask) / m) * left_impurity \\\n",
    "                                         - (np.sum(right_mask) / m) * right_impurity\n",
    "\n",
    "                    # Update the best split if needed\n",
    "                    if impurity_reduction > best_impurity_reduction:\n",
    "                        best_impurity_reduction = impurity_reduction\n",
    "                        best_split = (feature_index, threshold)\n",
    "\n",
    "        return best_split\n",
    "\n",
    "    def _calculate_impurity(self, y):\n",
    "        _, counts = np.unique(y, return_counts=True)\n",
    "        probabilities = counts / len(y)\n",
    "        impurity = 1 - np.sum(probabilities ** 2)\n",
    "        return impurity\n",
    "\n",
    "    def predict(self, X):\n",
    "        predictions = [self._predict_single(x) for x in X]\n",
    "        return np.array(predictions)\n",
    "\n",
    "    def _predict_single(self, x, node=None):\n",
    "        if node is None:\n",
    "            node = self.tree\n",
    "\n",
    "        if 'class' in node:\n",
    "            return node['class']\n",
    "        else:\n",
    "            if x[node['feature_index']] <= node['threshold']:\n",
    "                return self._predict_single(x, node['left'])\n",
    "            else:\n",
    "                return self._predict_single(x, node['right'])\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Create and train the Decision Tree model\n",
    "tree_model = DecisionTree(max_depth=3)\n",
    "tree_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = tree_model.predict(X_test)\n",
    "print((\"The predicted value using Decision tree{}\"),y_pred)\n",
    "# Evaluate the accuracy of the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracyof the model:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a3e2f65f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The predicted value using SVM{} [-1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
      "The Accuracy of the model: 0.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class SVM:\n",
    "    def __init__(self, C=1.0):\n",
    "        self.C = C\n",
    "        self.alpha = None\n",
    "        self.b = None\n",
    "\n",
    "    def _linear_kernel(self, X1, X2):\n",
    "        return np.dot(X1, X2.T)\n",
    "\n",
    "    def _calculate_error(self, X, y, alpha, b, kernel):\n",
    "        predictions = np.dot((alpha * y).T, kernel(X, X)) + b\n",
    "        errors = predictions - y\n",
    "        return errors\n",
    "\n",
    "    def _select_random_pair(self, m, i):\n",
    "        j = i\n",
    "        while j == i:\n",
    "            j = np.random.randint(m)\n",
    "        return j\n",
    "\n",
    "    def _clip_alpha(self, alpha, L, H):\n",
    "        return max(L, min(alpha, H))\n",
    "    \n",
    "    \n",
    "    def _take_step(self, i, j, X, y, alpha, b, errors, kernel):\n",
    "        if i == j:\n",
    "            return 0, alpha, b\n",
    "\n",
    "        alpha_i, alpha_j = alpha[i], alpha[j]\n",
    "        y_i, y_j = y[i], y[j]\n",
    "        E_i, E_j = self._calculate_error(X, y, alpha, b, kernel)[i], self._calculate_error(X, y, alpha, b, kernel)[j]\n",
    "\n",
    "    # Compute L and H\n",
    "        if y_i != y_j:\n",
    "            L = max(0, alpha_j - alpha_i)\n",
    "            H= min(self.C, self.C + alpha_j - alpha_i)\n",
    "        else:\n",
    "            L = max(0, alpha_i + alpha_j - self.C)\n",
    "            H = min(self.C, alpha_i + alpha_j)\n",
    "\n",
    "        if L == H:\n",
    "            return 0, alpha, b\n",
    "\n",
    "    # Compute kernel values\n",
    "        k_ij = kernel(X[i], X[i]) + kernel(X[j], X[j]) - 2 * kernel(X[i], X[j])\n",
    "\n",
    "        if k_ij <= 0:\n",
    "            return 0, alpha, b\n",
    "\n",
    "    # Update alpha_j\n",
    "        new_alpha_j = alpha_j + y_j * (E_i - E_j) / k_ij\n",
    "        new_alpha_j = self._clip_alpha(new_alpha_j, L, H)\n",
    "\n",
    "        if np.abs(new_alpha_j - alpha_j) < 1e-5:\n",
    "            return 0, alpha, b\n",
    "\n",
    "    # Update alpha_i\n",
    "        new_alpha_i = alpha_i + y_i * y_j * (alpha_j - new_alpha_j)\n",
    "\n",
    "    # Update b\n",
    "        b_i = E_i + y_i * (new_alpha_i - alpha_i) * kernel(X[i], X[i]) \\\n",
    "              + y_j * (new_alpha_j - alpha_j) * kernel(X[i], X[j]) + b\n",
    "        b_j = E_j + y_i * (new_alpha_i - alpha_i) * kernel(X[i], X[j]) \\\n",
    "              + y_j * (new_alpha_j - alpha_j) * kernel(X[j], X[j]) + b\n",
    "        b = (b_i + b_j) / 2\n",
    "\n",
    "    # Update alpha values\n",
    "        alpha[i] = new_alpha_i\n",
    "        alpha[j] = new_alpha_j\n",
    "\n",
    "        return 1, alpha, b\n",
    "\n",
    "\n",
    "    \n",
    "    def fit(self, X, y, max_iter=100):\n",
    "        m, n = X.shape\n",
    "        self.alpha = np.zeros(m)\n",
    "        self.b = 0\n",
    "        self.X_train = X  # Add this line to store X_train\n",
    "        self.y_train = y  # Add this line to store y_train\n",
    "\n",
    "        kernel = self._linear_kernel\n",
    "\n",
    "        for _ in range(max_iter):\n",
    "            alpha_changed = 0\n",
    "            for i in range(m):\n",
    "                E_i = self._calculate_error(X, y, self.alpha, self.b, kernel)[i]\n",
    "                if ((y[i] * E_i < -1e-5) and (self.alpha[i] < self.C)) or \\\n",
    "                      ((y[i] * E_i > 1e-5) and (self.alpha[i] > 0)):\n",
    "                    j = self._select_random_pair(m, i)\n",
    "                    change, self.alpha, self.b = self._take_step(i, j, X, y, self.alpha, self.b, E_i, kernel)\n",
    "                    alpha_changed += change\n",
    "\n",
    "            if alpha_changed == 0:\n",
    "                break\n",
    "\n",
    "        return self.alpha, self.b\n",
    "\n",
    "    def predict(self, X, alpha, b):\n",
    "        kernel = self._linear_kernel\n",
    "        predictions = np.dot(alpha * self.y_train, kernel(X, self.X_train).T) + b\n",
    "\n",
    "        return np.sign(predictions)\n",
    "\n",
    "# Convert labels to binary for a two-class SVM\n",
    "y_binary = np.where(y == 0, -1, 1)\n",
    "\n",
    "# Create and train the SVM model\n",
    "svm_model = SVM(C=1.0)\n",
    "alpha, b = svm_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = svm_model.predict(X_test, alpha, b)\n",
    "print((\"The predicted value using SVM{}\"),y_pred)\n",
    "\n",
    "# Evaluate the accuracy of the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"The Accuracy of the model:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c8869057",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The predicted value using Linear Regression{} [ 1.23071715 -0.04010441  2.21970287  1.34966889  1.28429336  0.02248402\n",
      "  1.05726124  1.82403704  1.36824643  1.06766437  1.70031437 -0.07357413\n",
      " -0.15562919 -0.06569402 -0.02128628  1.39659966  2.00022876  1.04812731\n",
      "  1.28102792  1.97283506  0.03184612  1.59830192  0.09450931  1.91807547\n",
      "  1.83296682  1.87877315  1.78781234  2.03362373  0.03594506  0.02619043]\n",
      "Mean Squared Error: 0.03711379440797648\n"
     ]
    }
   ],
   "source": [
    "class LinearRegression:\n",
    "    def __init__(self):\n",
    "        self.theta = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        X_b = np.c_[np.ones((X.shape[0], 1)), X]  # Add a bias term (intercept)\n",
    "        self.theta = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y)\n",
    "\n",
    "    def predict(self, X):\n",
    "        X_b = np.c_[np.ones((X.shape[0], 1)), X]\n",
    "        return X_b.dot(self.theta)\n",
    "\n",
    "# Train the linear regression model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "print((\"The predicted value using Linear Regression{}\"),y_pred)\n",
    "\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Mean Squared Error:\", mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "15caf58a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson Correlation Coefficient: -0.11756978413300201\n"
     ]
    }
   ],
   "source": [
    "# Define a function to calculate the Pearson correlation coefficient\n",
    "def pearson_correlation(x, y):\n",
    "    n = len(x)\n",
    "\n",
    "    mean_x = np.mean(x)\n",
    "    mean_y = np.mean(y)\n",
    "\n",
    "    numerator = np.sum((x - mean_x) * (y - mean_y))\n",
    "    denominator_x = np.sum((x - mean_x)**2)\n",
    "    denominator_y = np.sum((y - mean_y)**2)\n",
    "\n",
    "    correlation = numerator / np.sqrt(denominator_x * denominator_y)\n",
    "\n",
    "    return correlation\n",
    "\n",
    "# Calculate the Pearson correlation coefficient between two columns of the Iris dataset\n",
    "feature1 = X[:, 0]  # Choose the first feature\n",
    "feature2 = X[:, 1]  # Choose the second feature\n",
    "\n",
    "correlation_coefficient = pearson_correlation(feature1, feature2)\n",
    "print(f\"Pearson Correlation Coefficient: {correlation_coefficient}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3a025b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def fisher_discriminant_ratio(X, y):\n",
    "    # Calculate class means\n",
    "    class_means = []\n",
    "    classes = np.unique(y)\n",
    "    for c in classes:\n",
    "        class_means.append(np.mean(X[y == c], axis=0))\n",
    "    \n",
    "    # Calculate the overall mean\n",
    "    overall_mean = np.mean(X, axis=0)\n",
    "    \n",
    "    # Calculate within-class scatter matrix (Sw) and between-class scatter matrix (Sb)\n",
    "    S_within = np.zeros((X.shape[1], X.shape[1]))\n",
    "    S_between = np.zeros((X.shape[1], X.shape[1]))\n",
    "    for i, c_mean in zip(classes, class_means):\n",
    "        # Within-class scatter matrix\n",
    "        diff_within = X[y == i] - c_mean\n",
    "        S_within += np.dot(diff_within.T, diff_within)\n",
    "        \n",
    "        # Between-class scatter matrix\n",
    "        diff_between = (c_mean - overall_mean).reshape(-1, 1)\n",
    "        S_between += np.dot(diff_between, diff_between.T)\n",
    "    \n",
    "    # Calculate eigenvalues and eigenvectors of Sw^-1 * Sb\n",
    "    eigen_values, eigen_vectors = np.linalg.eig(np.dot(np.linalg.inv(S_within), S_between))\n",
    "    \n",
    "    # Sort eigenvalues and eigenvectors in descending order\n",
    "    idx = np.argsort(eigen_values)[::-1]\n",
    "    eigen_values = eigen_values[idx]\n",
    "    eigen_vectors = eigen_vectors[:, idx]\n",
    "    \n",
    "    return eigen_vectors[:, 0]  # Return the eigenvector corresponding to the largest eigenvalue\n",
    "\n",
    "# Example usage:\n",
    "# Assuming X is your feature matrix and y is the corresponding labels\n",
    "# X and y should be appropriately prepared before using this function\n",
    "# eigenvector = fisher_discriminant_ratio(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc29cdc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
